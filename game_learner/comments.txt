Tic-tac-toe
* For full exploration (no explotation), one problem is it will still choose invalid moves a lot of the time.  Also, the Q-values will be highly unstable, since the AI are playing randomly, so they will be optimizing against a random strategy, not an optimal strategy.  For full exploitation, there are the usual caveats.
* For the learning rate, it is tempting to set it to 1.  But this can make the AI learn to favor a bad strategy when its opponent happened to play poorly the last time, and vice versa.  So an intermediate learning rate might still be best.
* I thought about just retraining Q instead of updating Q in each iteration.  But this won't save any time.  What it can do is erase "early noise" but it it might make convergence slower.
* I think it might be important to keep the learning rate low, since it's possible for the oppoenent AI to randomly play poorly which might throw off the Q value.  For example, if X is about to win, and O doesn't block it, but then X derps out and doesn't claim the win, then O blocks it and eventually wins, O might learn that not blocking that move was good.
* Moreover, the algorithm has many many chances to update its weights, so a low learning rate should not be a concern.
* This basically bore out in the current best Q function which I ran for 500 iterations of 64 episodes, with learning rate 0.1 and greed 0.5.  I believe this is optimal.  I tried tweaking a bunch of the other parameters and this one seemed to make the biggest most obvious difference.  Previously, the models would mostly work but have some "gaps" where it would lose in really dumb ways.
* The basic test case is to play as first player, play in a corner.  It should play center.  Then play opposite corner.  Then look at its Q function.  It should clearly differentiate between corners and non-corners.  Comparing greed of 0.5 with 64 episodes and (A) rate of 0.2 and 1000 iterations and (B) rate of 0.1 and 500 iterations, (B) did this better.
* Even the losing strategies have some small positive value.  This is because the opponent still behaves randomly sometimes, so occasionally it will win playing there and then update the strategy accordingly.  We can try to further differentiate these two (winning vs. losing strategy) cases by making the algorithm more greedy.  I increased the greed to 0.8, keeping everything else the same as (B).  Concerns with doing this are: the AI might not sufficiently explore the tree if the iterations aren't enough, i.e. you might be able to beat it by playing a suboptimal strategy to get it in a place where it doesn't know the optimal stategy.
* The result was, for data (A) is greed 0.5 and (B) is greed 0.8:

SE/NW orientation: (A) corners=.23,.18; sides=.55,.50,.53,.55 (B) corners=.16,-.01; sides=.24,.22,.20,.17
SW/NE orientation: (A) corners=.37,-.13; sides=.50,.68,.73,.61 (B) corners=-.35,-.23; sides=.28,.30,.19,.39

The Q-values generally got lower.  Somtimes they became more differentiated, and sometimes less differentiated for some orientations (but was about the same for others), likely because the there wasn't enough random play and iterations to explore parts of the tree that were equivalent by symmetry but not chosen to be "optimal" by chance.  The result is a more imbalanced Q-function.  The resulting policy still seems optimal.

* Roughly speaking, it seems we want the learning rate low to guarantee convergence.  Setting the greed too high carries risks of underexploring.

* I had trouble getting the deep Connect Four network to actually learn things, so I tried it on tic-tac-toe.  I tried: replacing ReLU with LeakyReLU, adding more layers, scaling the greed, imposing a memory threshold for training.  Tried making it update less but that made it worse.  I plotted a graph of the losses and indeed they are diverging.  The fix ended up being: I had imposed a large penalty, like -1000 (vs. -1 for a loss) for playing illegally to discourage it, which was skewing the weights.  I changed it to -1 and the model was close to optimal after 15 minutes of training.  I called this model "almostoptimal".
* I bumped the learning rate up from 0.0001 to 0.001 and ran it again.  It improved on one case, but still not optimal, and still sometimes tries to make illegal moves (one of which results in a loss).  I called this model "almostoptimal2".
* I noticed that the loss would dive down but then slowly start to climb.  I trained another model, this time stopping where the loss minimized.  The resulting AI was quite bad.  Possible reason: the loss decreases as the policy network learns to play against its own weights, but climbs as it starts to learn against itself.  When the learning rate was 0.0001, the loss didn't rise and kept going down.
* Tried 0.0005, the loss seemed to converge but it failed the corner test unfortunately.
* Tried 0.001 and upped the penalty to -2.  This decreased the number of penalties but still the AI isn't playing optimally.  I retrained it once barely changing the parameters and it became optimal.  I trained it again, the exact same parameters, and it sometimes didn't take winning strategies as player 2 (going for ties).  [Side observation: the deep AI tends to play center as player 2, while the classical AI tends to play side.]  Looking at the loss graphs, player 2 for the non-optimal AI had a higher loss and didn't seem to be converging as quickly.

* Revisited classical Q-learning.  You get can an optimal AI with learning rate 0.01 and 300 iterations of 64 episodes (middle greed) which takes about 2 minutes.  Interestingly, the losses for player 1 rise then dive, but the losses for player 2 rise then stay flat.  It's still converging, just to a higher value than its initial.  One reason they don't both converge to 0 is due to randomness in the greedy strategy.  It's interesting that the graphs here are smoother than for NNs, but it might just be because we do fewer updates.  Based on the graph for 500 iterations at 0.1 learning rate, it seems like the place to stop might just be 250-300 iterations, though the difference at that point is marginal.

* I wonder if I can get the NN to be beat this with less training, but so far I can't.



Connect Four:
* 100 episodes, 1000 iterations is taking maybe 2 hours.  The AI is not great because there are large branches of the tree it hasn't explored at all.  Will try training it with a neural network.
* I wrote the code.  A few initial questions.
** I wrote the code using PyTorch tensors rather than NumPy tensors.  I guess the former can be run on GPUs, while the latter is optimized for CPUs?  I wonder if I should be doing the latter.
** I did an experiment where I just ran updates on the same play on an empty board 500 times with a learn rate of 0.01.  What should happen is that the Q-value should converge to the value of the next state.  Instead, what happened is that they would explode.  I was using the Adam (or AdamW) optimizer and had some batch normalization in the neural network.  I was able to eliminate this by changing the optimizer to SGD and removing batch normalization.  MSE and Huber loss both worked fine, with Huber converging maybe a little faster.  I was able to get RMSProp to work by lowering the learn rate significantly to 0.00025.  Same with Adam.  Batch normalization still caused issues.
