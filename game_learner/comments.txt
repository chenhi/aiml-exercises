Tic-tac-toe
* For full exploration (no explotation), one problem is it will still choose invalid moves a lot of the time.  Also, the Q-values will be highly unstable, since the AI are playing randomly, so they will be optimizing against a random strategy, not an optimal strategy.  For full exploitation, there are the usual caveats.
* For the learning rate, it is tempting to set it to 1.  But this can make the AI learn to favor a bad strategy when its opponent happened to play poorly the last time, and vice versa.  So an intermediate learning rate might still be best.
* I thought about just retraining Q instead of updating Q in each iteration.  But this won't save any time.  What it can do is erase "early noise" but it it might make convergence slower.
* I think it might be important to keep the learning rate low, since it's possible for the oppoenent AI to randomly play poorly which might throw off the Q value.  For example, if X is about to win, and O doesn't block it, but then X derps out and doesn't claim the win, then O blocks it and eventually wins, O might learn that not blocking that move was good.
* Moreover, the algorithm has many many chances to update its weights, so a low learning rate should not be a concern.
* This basically bore out in the current best Q function which I ran for 500 iterations of 64 episodes, with learning rate 0.1 and greed 0.5.  I believe this is optimal.  I tried tweaking a bunch of the other parameters and this one seemed to make the biggest most obvious difference.  Previously, the models would mostly work but have some "gaps" where it would lose in really dumb ways.
* The basic test case is to play as first player, play in a corner.  It should play center.  Then play opposite corner.  Then look at its Q function.  It should clearly differentiate between corners and non-corners.  Comparing greed of 0.5 with 64 episodes and (A) rate of 0.2 and 1000 iterations and (B) rate of 0.1 and 500 iterations, (B) did this better.
* Even the losing strategies have some small positive value.  This is because the opponent still behaves randomly sometimes, so occasionally it will win playing there and then update the strategy accordingly.  We can try to further differentiate these two (winning vs. losing strategy) cases by making the algorithm more greedy.  I increased the greed to 0.8, keeping everything else the same as (B).  Concerns with doing this are: the AI might not sufficiently explore the tree if the iterations aren't enough, i.e. you might be able to beat it by playing a suboptimal strategy to get it in a place where it doesn't know the optimal stategy.
* The result was, for data (A) is greed 0.5 and (B) is greed 0.8:

SE/NW orientation: (A) corners=.23,.18; sides=.55,.50,.53,.55 (B) corners=.16,-.01; sides=.24,.22,.20,.17
SW/NE orientation: (A) corners=.37,-.13; sides=.50,.68,.73,.61 (B) corners=-.35,-.23; sides=.28,.30,.19,.39

The Q-values generally got lower.  Somtimes they became more differentiated, and sometimes less differentiated for some orientations (but was about the same for others), likely because the there wasn't enough random play and iterations to explore parts of the tree that were equivalent by symmetry but not chosen to be "optimal" by chance.  The result is a more imbalanced Q-function.  The resulting policy still seems optimal.

* Roughly speaking, it seems we want the learning rate low to guarantee convergence.  Setting the greed too high carries risks of underexploring.

* I had trouble getting the deep Connect Four network to actually learn things, so I tried it on tic-tac-toe.  I tried: replacing ReLU with LeakyReLU, adding more layers, scaling the greed, imposing a memory threshold for training.  Tried making it update less but that made it worse.  I plotted a graph of the losses and indeed they are diverging.  The fix ended up being: I had imposed a large penalty, like -1000 (vs. -1 for a loss) for playing illegally to discourage it, which was skewing the weights.  I changed it to -1 and the model was close to optimal after 15 minutes of training.  I called this model "almostoptimal".
* I bumped the learning rate up from 0.0001 to 0.001 and ran it again.  It improved on one case, but still not optimal, and still sometimes tries to make illegal moves (one of which results in a loss).  I called this model "almostoptimal2".
* I noticed that the loss would dive down but then slowly start to climb.  I trained another model, this time stopping where the loss minimized.  The resulting AI was quite bad.  Possible reason: the loss decreases as the policy network learns to play against its own weights, but climbs as it starts to learn against itself.  When the learning rate was 0.0001, the loss didn't rise and kept going down.
* Tried 0.0005, the loss seemed to converge but it failed the corner test unfortunately.
* Tried 0.001 and upped the penalty to -2.  This decreased the number of penalties but still the AI isn't playing optimally.  I retrained it once barely changing the parameters and it became optimal.  I trained it again, the exact same parameters, and it sometimes didn't take winning strategies as player 2 (going for ties).  [Side observation: the deep AI tends to play center as player 2, while the classical AI tends to play side.]  Looking at the loss graphs, player 2 for the non-optimal AI had a higher loss and didn't seem to be converging as quickly.

* Revisited classical Q-learning.  You get can an optimal AI with learning rate 0.01 and 300 iterations of 64 episodes (middle greed) which takes about 2 minutes.  Interestingly, the losses for player 1 rise then dive, but the losses for player 2 rise then stay flat.  It's still converging, just to a higher value than its initial.  One reason they don't both converge to 0 is due to randomness in the greedy strategy.  It's interesting that the graphs here are smoother than for NNs, but it might just be because we do fewer updates.  Based on the graph for 500 iterations at 0.1 learning rate, it seems like the place to stop might just be 250-300 iterations, though the difference at that point is marginal.

* I wonder if I can get the NN to be beat this with less training, but so far I can't.

* In some graphs I noticed that there was often a divergence at the end, and that this divergence was independent of the number of iterations (i.e. it happened at whatever scale).  It turns out this was because I was scaling the greed in the following way: initially, the greed starts at the minimum, then it attenuates to the maximum over the full interval.  I thought the point was to generate a random data set to begin with and then ramp up the exploitation.  But actually it doesn't make sense: the random data set isn't that valuable and gets generated anyway, and this doesn't allow time at the end for the network to adjust to the new greed values.  In fact this is the reverse of what DQN prescribes: rising greed until the end, where it is constant at its final value for some time.  Also, the DQN paper suggested ending greed of 0.9, but this is too high for multiplayer games, which are more dynamic; it can cause the bots to get stuck in fixed branches of the game tree.  The result is, e.g., it might play optimally in one case but then in a symmetric case which it never explored, it does not.  I ended the greed around 0.5.

* I also misunderstood the role of the target network.  I thought it was used to get the action during trianing, but it is only used to compute the "y" value.  It's still unclear to me why this is desirable.  I fixed it and the algorithm did seem more stable, with less random bumps in the loss curve (after smoothing).   I think the way I had it previously set up, the policy network basically played no role during training.  It was the one being trained, but all selection was being done by the target network.  Maybe this doesn't make sense; one might as well generate a ton of data using the network, then do all the training at once (instead of copying it over).

* I noticed that the NN AI was still failing the "side vs. corners" test case from above, i.e. not distinguishing the cases very well.  I tried bumping up the simulation batches from 4 to 16 and the training batches from 64 to 128 and this immediately improved it.  I guessed that it was probably the simulation batches going up that helped; this meant it could explore more branches of the tree.  I then ran a test only bumping up the training batches.  This actually made performance worse than the original.  I ran a test bumping up only the simulation batches and it was still not optimal.  So I think maybe both of these just need to be high enough: we need enough simulations to explore enough of the tree each "update", and we need enough training to be done so that these branches are sufficiently sampled.

Connect Four:
* 100 episodes, 1000 iterations is taking maybe 2 hours.  The AI is not great because there are large branches of the tree it hasn't explored at all.  Will try training it with a neural network.
* I wrote the code.  A few initial questions.
** I wrote the code using PyTorch tensors rather than NumPy tensors.  I guess the former can be run on GPUs, while the latter is optimized for CPUs?  I wonder if I should be doing the latter.
** I did an experiment where I just ran updates on the same play on an empty board 500 times with a learn rate of 0.01.  What should happen is that the Q-value should converge to the value of the next state.  Instead, what happened is that they would explode.  I was using the Adam (or AdamW) optimizer and had some batch normalization in the neural network.  I was able to eliminate this by changing the optimizer to SGD and removing batch normalization.  MSE and Huber loss both worked fine, with Huber converging maybe a little faster.  I was able to get RMSProp to work by lowering the learn rate significantly to 0.00025.  Same with Adam.  Batch normalization still caused issues.
