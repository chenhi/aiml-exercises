Tic-tac-toe
* For full exploration (no explotation), one problem is it will still choose invalid moves a lot of the time.  Also, the Q-values will be highly unstable, since the AI are playing randomly, so they will be optimizing against a random strategy, not an optimal strategy.  For full exploitation, there are the usual caveats.
* For the learning rate, it is tempting to set it to 1.  But this can make the AI learn to favor a bad strategy when its opponent happened to play poorly the last time, and vice versa.  So an intermediate learning rate might still be best.
* I thought about just retraining Q instead of updating Q in each iteration.  But this won't save any time.  What it can do is erase "early noise" but it it might make convergence slower.
* I think it might be important to keep the learning rate low, since it's possible for the oppoenent AI to randomly play poorly which might throw off the Q value.  For example, if X is about to win, and O doesn't block it, but then X derps out and doesn't claim the win, then O blocks it and eventually wins, O might learn that not blocking that move was good.
* Moreover, the algorithm has many many chances to update its weights, so a low learning rate should not be a concern.
* This basically bore out in the current best Q function which I ran for 500 iterations of 64 episodes, with learning rate 0.1 and greed 0.5.  I believe this is optimal.  I tried tweaking a bunch of the other parameters and this one seemed to make the biggest most obvious difference.  Previously, the models would mostly work but have some "gaps" where it would lose in really dumb ways.
* The basic test case is to play as first player, play in a corner.  It should play center.  Then play opposite corner.  Then look at its Q function.  It should clearly differentiate between corners and non-corners.  Comparing greed of 0.5 with 64 episodes and (A) rate of 0.2 and 1000 iterations and (B) rate of 0.1 and 500 iterations, (B) did this better.
* Even the losing strategies have some small positive value.  This is because the opponent still behaves randomly sometimes, so occasionally it will win playing there and then update the strategy accordingly.  We can try to further differentiate these two (winning vs. losing strategy) cases by making the algorithm more greedy.  I increased the greed to 0.8, keeping everything else the same as (B).  Concerns with doing this are: the AI might not sufficiently explore the tree if the iterations aren't enough, i.e. you might be able to beat it by playing a suboptimal strategy to get it in a place where it doesn't know the optimal stategy.
* The result was, for data (A) is greed 0.5 and (B) is greed 0.8:

SE/NW orientation: (A) corners=.23,.18; sides=.55,.50,.53,.55 (B) corners=.16,-.01; sides=.24,.22,.20,.17
SW/NE orientation: (A) corners=.37,-.13; sides=.50,.68,.73,.61 (B) corners=-.35,-.23; sides=.28,.30,.19,.39

The Q-values generally got lower.  Somtimes they became more differentiated, and sometimes less differentiated for some orientations (but was about the same for others), likely because the there wasn't enough random play and iterations to explore parts of the tree that were equivalent by symmetry but not chosen to be "optimal" by chance.  The result is a more imbalanced Q-function.  The resulting policy still seems optimal.

* Roughly speaking, it seems we want the learning rate low to guarantee convergence.  Setting the greed too high carries risks of underexploring.
* Some of these lessons might not carry over to training with a neural network.

Connect Four:
* 100 episodes, 1000 iterations is taking maybe 2 hours.  The AI is not great because there are large branches of the tree it hasn't explored at all.  Will try training it with a neural network.
* I wrote the code.  A few initial questions.
** I wrote the code using PyTorch tensors rather than NumPy tensors.  I guess the former can be run on GPUs, while the latter is optimized for CPUs?  I wonder if I should be doing the latter.
** I did an experiment where I just ran updates on the same play on an empty board 500 times.  What should happen is that the Q-value should converge to the value of the next state.  Instead, what happened is that they would explode.  I was using the Adam (or AdamW) optimizer and had some batch normalization in the neural network.  I was able to eliminate this by changing the optimizer to SGD and removing batch normalization.  MSE and Huber loss both worked fine, with Huber converging maybe a little faster.
