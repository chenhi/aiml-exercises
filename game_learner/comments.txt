Tic-tac-toe
* For full exploration (no explotation), one problem is it will still choose invalid moves a lot of the time.  Also, the Q-values will be highly unstable, since the AI are playing randomly, so they will be optimizing against a random strategy, not an optimal strategy.  For full exploitation, there are the usual caveats.
* For the learning rate, it is tempting to set it to 1.  But this can make the AI learn to favor a bad strategy when its opponent happened to play poorly the last time, and vice versa.  So an intermediate learning rate might still be best.
* I thought about just retraining Q instead of updating Q in each iteration.  But this won't save any time.  What it can do is erase "early noise" but it it might make convergence slower.
* I wonder if I should make the algorithm a bit more greedy.  I can play with this a little bit.
* Somehow, when I ran for more iterations but shorter episodes (1000/10), the performance got a lot worse (even though it took longer to train) than 100/100.
* Generally, player 2 is a lot worse than player 1 for 100/100.

* Think about: penalizing invalid moves vs. prohibiting them (though it's not strictly prohibited, only in training).

Connect Four:
* 100 episodes, 1000 iterations is taking maybe 2 hours.  The AI is not great because there are large branches of the tree it hasn't explored at all.  Will try training it with a neural network.
